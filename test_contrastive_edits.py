#!/usr/bin/env python3
"""
Quick Test Script for MedCalc Contrastive Edits Pipeline

This script runs a minimal test to verify the pipeline works correctly.
"""

import sys
import os
from pathlib import Path

# Add the main script to the path
sys.path.insert(0, str(Path(__file__).parent))

from medcalc_with_contrastive_boosted_edits import MedCalcContrastiveEvaluationPipeline

def test_pipeline():
    print("üß™ Testing MedCalc Contrastive Edits Pipeline")
    print("="*60)
    
    # Check virtual environment
    if not hasattr(sys, 'real_prefix') and not (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        print("\n‚ö†Ô∏è  Warning: Virtual environment not detected!")
        print("   It's recommended to activate the virtual environment:")
        print("   source ../mohs-llm-as-a-judge/llm-judge-env/bin/activate")
        response = input("\n   Continue anyway? (y/n): ").strip().lower()
        if response != 'y':
            return False
    else:
        print("\n‚úÖ Virtual environment active")
    
    # Check prerequisites
    print("\n1Ô∏è‚É£ Checking Prerequisites...")
    
    if not Path("MedCalc-Bench").exists():
        print("‚ùå MedCalc-Bench directory not found!")
        return False
    print("   ‚úì MedCalc-Bench directory found")
    
    if not Path("MedCalc-Bench/dataset/train_data.csv").exists():
        print("‚ùå train_data.csv not found! Run: unzip MedCalc-Bench/dataset/train_data.csv.zip")
        return False
    print("   ‚úì train_data.csv found")
    
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("‚ùå OPENAI_API_KEY not set!")
        return False
    print("   ‚úì OPENAI_API_KEY found")
    
    # Test initialization
    print("\n2Ô∏è‚É£ Testing Pipeline Initialization...")
    try:
        pipeline = MedCalcContrastiveEvaluationPipeline(
            api_key=api_key,
            sample_size=2,  # Very small sample for quick test
            model="OpenAI/gpt-4o"
        )
        print("   ‚úì Pipeline initialized successfully")
    except Exception as e:
        print(f"   ‚ùå Initialization failed: {e}")
        return False
    
    # Test data loading
    print("\n3Ô∏è‚É£ Testing Data Loading...")
    try:
        df = pipeline.load_medcalc_data(sample_size=2)
        if len(df) == 2:
            print(f"   ‚úì Loaded 2 samples successfully")
        else:
            print(f"   ‚ö†Ô∏è  Expected 2 samples, got {len(df)}")
    except Exception as e:
        print(f"   ‚ùå Data loading failed: {e}")
        return False
    
    # Test prompt creation
    print("\n4Ô∏è‚É£ Testing Enhanced Prompt Creation...")
    try:
        first_calculator_id = str(df.iloc[0]["Calculator ID"])
        example = pipeline.one_shot_examples.get(first_calculator_id)
        
        if example:
            system_msg, user_msg = pipeline.extract_original_one_shot_prompt(
                "Sample note", "Sample question",
                example["Patient Note"],
                {"step_by_step_thinking": example["Response"]["step_by_step_thinking"], 
                 "answer": example["Response"]["answer"]}
            )
            original_prompt = system_msg + "\n\n" + user_msg
            print(f"   ‚úì Original prompt extracted ({len(original_prompt)} chars)")
            
            enhanced_prompts = pipeline.create_enhanced_prompts(original_prompt)
            if 'chain_of_thought' in enhanced_prompts and 'chain_of_draft' in enhanced_prompts:
                print(f"   ‚úì Enhanced prompts created:")
                print(f"      ‚Ä¢ CoT: {len(enhanced_prompts['chain_of_thought']['prompt'])} chars")
                print(f"      ‚Ä¢ CoD: {len(enhanced_prompts['chain_of_draft']['prompt'])} chars")
            else:
                print("   ‚ö†Ô∏è  Enhanced prompts missing expected techniques")
        else:
            print("   ‚ö†Ô∏è  No one-shot example found for first calculator")
    except Exception as e:
        print(f"   ‚ùå Prompt creation failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # Test response generation (single sample to avoid API costs)
    print("\n5Ô∏è‚É£ Testing Response Generation (1 sample)...")
    print("   ‚ö†Ô∏è  This will make 3 API calls (costs ~$0.03-0.05)")
    
    response = input("   Continue with API test? (y/n): ").strip().lower()
    if response != 'y':
        print("   ‚è≠Ô∏è  Skipping API test")
        print("\n‚úÖ Basic tests passed! Pipeline structure is correct.")
        return True
    
    try:
        # Test with just one sample from the loaded data
        single_sample_df = df.head(1)
        all_responses = pipeline.generate_responses(single_sample_df, enhanced_prompts)
        
        if len(all_responses) == 3:
            print(f"   ‚úì Generated responses for 3 prompt types")
            for prompt_type, responses in all_responses.items():
                if len(responses) > 0:
                    print(f"      ‚Ä¢ {prompt_type}: {len(responses)} response(s)")
                else:
                    print(f"      ‚ö†Ô∏è  {prompt_type}: No responses generated")
        else:
            print(f"   ‚ö†Ô∏è  Expected 3 prompt types, got {len(all_responses)}")
    except Exception as e:
        print(f"   ‚ùå Response generation failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # Test evaluation
    print("\n6Ô∏è‚É£ Testing Evaluation...")
    try:
        evaluation_results = pipeline.evaluate_and_split_responses(all_responses)
        print(f"   ‚úì Evaluation completed:")
        for prompt_type, results in evaluation_results.items():
            print(f"      ‚Ä¢ {prompt_type}: {results['accuracy']:.0%} ({results['correct']}/{results['total']})")
    except Exception as e:
        print(f"   ‚ùå Evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # Verify output files
    print("\n7Ô∏è‚É£ Verifying Output Files...")
    try:
        output_dir = pipeline.output_dir
        expected_files = [
            "responses/original_responses.jsonl",
            "responses/chain_of_thought_responses.jsonl",
            "responses/chain_of_draft_responses.jsonl",
            "evaluations/evaluation_summary.json",
            "correct/original_correct.jsonl",
            "incorrect/original_incorrect.jsonl"
        ]
        
        for file_path in expected_files:
            full_path = output_dir / file_path
            if full_path.exists():
                print(f"   ‚úì {file_path}")
            else:
                print(f"   ‚ö†Ô∏è  {file_path} not found")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  File verification failed: {e}")
    
    print("\n" + "="*60)
    print("‚úÖ ALL TESTS PASSED!")
    print(f"\nOutput directory: {pipeline.output_dir}")
    print("\nYou can now run the full pipeline with:")
    print("  python medcalc_with_contrastive_boosted_edits.py --sample-size 500")
    
    return True


if __name__ == "__main__":
    success = test_pipeline()
    sys.exit(0 if success else 1)

